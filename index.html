<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>VIMA | General Robot Manipulation with Multimodal Prompts</title>

    <script>
        var task_map = {
            "simple-object-manipulation": "simple_object_manipulation",
            "visual-goal-reaching": "visual_goal_reaching",
            "novel-concept-grounding": "novel_concept_grounding",
            "one-shot-video-imitation": "one_shot_video_imitation",
            "visual-constraint-satisfaction": "visual_constraint_satisfaction",
            "visual-reasoning": "visual_reasoning"
        };

        function updateDemoVideo(category) {
            // var demo = document.getElementById("single-menu-demos").value;
            var task = document.getElementById(category + "-menu-tasks").value;
            var inst = document.getElementById(category + "-menu-instances").value;

            console.log(task_map[category], task, inst)

            var video = document.getElementById(category + "-single-task-video");
            video.src = "/media/videos/demos/" +
                task_map[category] +
                "/" +
                task +
                "/" +
                inst +
                ".mp4";
            video.playbackRate = 2.0;
            video.play();
        }
    </script>

    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
          rel="stylesheet">

    <link rel="stylesheet" href="./static/css/bulma.min.css">
    <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
    <link rel="stylesheet"
          href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="./static/css/index.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="./static/js/fontawesome.all.min.js"></script>
    <script src="./static/js/bulma-carousel.min.js"></script>
    <script src="./static/js/bulma-slider.min.js"></script>
    <script src="./static/js/index.js"></script>
</head>
<body>


<section class="hero">
    <div class="hero-body">
        <div class="container is-max-desktop">
            <div class="columns is-centered">
                <div class="column has-text-centered">
                    <h1 class="title is-1 publication-title">VIMA: General Robot Manipulation with Multimodal
                        Prompts</h1>
                    <div class="is-size-5 publication-authors">
            <span class="author-block">
                <a target="_blank" href="https://yunfanj.com/">Yunfan&#160;Jiang</a><sup>1 2</sup>,
                <a target="_blank" href="http://web.stanford.edu/~agrim/">Agrim&#160;Gupta</a><sup>2&dagger;</sup>,
                <a target="_blank" href="https://zcczhang.github.io/">Zichen&#160;Zhang</a><sup>3&dagger;</sup>,
                <a target="_blank" href="https://guanzhi.me/">Guanzhi&#160;Wang</a><sup>1 4&dagger;</sup>,
                <a target="_blank" href="">Yongqiang&#160;Dou</a><sup>5</sup>,
                <a target="_blank" href="">Yanjun&#160;Chen</a><sup>2</sup>,
                <br>
                <a target="_blank" href="https://profiles.stanford.edu/fei-fei-li">Li&#160;Fei-Fei</a><sup>2</sup>,
                <a target="_blank" href="http://tensorlab.cms.caltech.edu/users/anima/">Anima&#160;Anandkumar</a><sup>1 4</sup>,
                <a target="_blank" href="https://www.cs.utexas.edu/~yukez/">Yuke&#160;Zhu</a><sup>1 6&#8225;</sup>,
                <a target="_blank" href="https://jimfan.me/">Linxi&#160;"Jim"&#160;Fan</a><sup>1&#8225;</sup>
            </span>
                    </div>

                    <div class="is-size-5 publication-authors">
                        <span class="author-block"><sup>1</sup>NVIDIA, </span>
                        <span class="author-block"><sup>2</sup>Stanford, </span>
                        <span class="author-block"><sup>3</sup>Macalester College, </span>
                        <span class="author-block"><sup>4</sup>Caltech, </span>
                        <span class="author-block"><sup>5</sup>Tsinghua, </span>
                        <span class="author-block"><sup>6</sup>UT Austin</span>
                    </div>

                    <div class="is-size-5 publication-authors">
                        <span class="author-block"><sup>&dagger;</sup>Equal Contribution</span>
                        <span class="author-block"><sup>&#8225;</sup>Equal Advising </span>
                    </div>

                    <div class="column has-text-centered">
                        <div class="publication-links">
                            <!-- TODO PDF Link. -->
                            <span class="link-block">
                <a target="_blank" href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
                            <!-- Code Link. -->
                            <span class="link-block">
                <a target="_blank" href="https://github.com/vimalabs"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
                <a target="_blank" href="https://doi.org/10.5281/zenodo.7127587"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-database"></i>
                  </span>
                  <span>Data</span>
                  </a>
              </span>
                        </div>

                    </div>
                </div>
            </div>
        </div>
    </div>
</section>

<section class="hero is-light is-small">
    <div class="hero-body">
        <div class="container">
            <div id="results-carousel" class="carousel results-carousel">
                <div class="item item-sweep_without_exceeding">
                    <video poster="" id="sweep_without_exceeding" autoplay controls muted loop height="100%"
                           playbackRate=2.0>
                        <source src="/media/videos/demos/visual_constraint_satisfaction/sweep_without_exceeding/1.mp4"
                                type="video/mp4">
                    </video>
                </div>
                <div class="item item-sweep_without_touching">
                    <video poster="" id="sweep_without_touching" autoplay controls muted loop height="100%"
                           playbackRate=2.0>
                        <source src="/media/videos/demos/visual_constraint_satisfaction/sweep_without_touching/1.mp4"
                                type="video/mp4">
                    </video>
                </div>
                <div class="item item-rotate">
                    <video poster="" id="rotate" autoplay controls muted loop height="100%">
                        <source src="media/videos/demos/simple_object_manipulation/rotate/1.mp4"
                                type="video/mp4">
                    </video>
                </div>
                <div class="item item-scene_understanding">
                    <video poster="" id="scene_understanding" autoplay controls muted loop height="100%">
                        <source src="media/videos/demos/simple_object_manipulation/scene_understanding/1.mp4"
                                type="video/mp4">
                    </video>
                </div>
                <div class="item item-simple_manipulation">
                    <video poster="" id="simple_manipulation" autoplay controls muted loop height="100%">
                        <source src="media/videos/demos/simple_object_manipulation/simple_manipulation/1.mp4"
                                type="video/mp4">
                    </video>
                </div>
                <div class="item item-novel_adj">
                    <video poster="" id="novel_adj" autoplay controls muted loop height="100%">
                        <source src="media/videos/demos/novel_concept_grounding/novel_adj/1.mp4"
                                type="video/mp4">
                    </video>
                </div>
                <div class="item item-novel_adj_and_noun">
                    <video poster="" id="novel_adj_and_noun" autoplay controls muted loop height="100%">
                        <source src="media/videos/demos/novel_concept_grounding/novel_adj_and_noun/1.mp4"
                                type="video/mp4">
                    </video>
                </div>
                <div class="item item-novel_noun">
                    <video poster="" id="novel_noun" autoplay controls muted loop height="100%">
                        <source src="media/videos/demos/novel_concept_grounding/novel_noun/1.mp4"
                                type="video/mp4">
                    </video>
                </div>
                <div class="item item-twist">
                    <video poster="" id="twist" autoplay controls muted loop height="100%">
                        <source src="media/videos/demos/novel_concept_grounding/twist/1.mp4"
                                type="video/mp4">
                    </video>
                </div>
                <div class="item item-follow_motion">
                    <video poster="" id="follow_motion" autoplay controls muted loop height="100%">
                        <source src="media/videos/demos/one_shot_video_imitation/follow_motion/1.mp4"
                                type="video/mp4">
                    </video>
                </div>
                <div class="item item-follow_order">
                    <video poster="" id="follow_order" autoplay controls muted loop height="100%">
                        <source src="media/videos/demos/one_shot_video_imitation/follow_order/1.mp4"
                                type="video/mp4">
                    </video>
                </div>
                <div class="item item-rearrange">
                    <video poster="" id="rearrange" autoplay controls muted loop height="100%">
                        <source src="media/videos/demos/visual_goal_reaching/rearrange/1.mp4"
                                type="video/mp4">
                    </video>
                </div>
                <div class="item item-manipulate_old_neighbor">
                    <video poster="" id="manipulate_old_neighbor" autoplay controls muted loop height="100%">
                        <source src="media/videos/demos/visual_reasoning/manipulate_old_neighbor/1.mp4"
                                type="video/mp4">
                    </video>
                </div>
                <div class="item item-pick_in_order_then_restore">
                    <video poster="" id="pick_in_order_then_restore" autoplay controls muted loop height="100%">
                        <source src="media/videos/demos/visual_reasoning/pick_in_order_then_restore/1.mp4"
                                type="video/mp4">
                    </video>
                </div>
                <div class="item item-rearrange_then_restore">
                    <video poster="" id="rearrange_then_restore" autoplay controls muted loop height="100%">
                        <source src="media/videos/demos/visual_goal_reaching/rearrange_then_restore/1.mp4"
                                type="video/mp4">
                    </video>
                </div>
                <div class="item item-same_color">
                    <video poster="" id="same_color" autoplay controls muted loop height="100%">
                        <source src="media/videos/demos/visual_reasoning/same_color/1.mp4"
                                type="video/mp4">
                    </video>
                </div>
                <div class="item item-same_profile">
                    <video poster="" id="same_profile" autoplay controls muted loop height="100%">
                        <source src="media/videos/demos/visual_reasoning/same_profile/1.mp4"
                                type="video/mp4">
                    </video>
                </div>
            </div>
        </div>
    </div>
</section>


<section class="section">
    <div class="container is-max-desktop">
        <!-- Abstract. -->
        <div class="columns is-centered has-text-centered">
            <div class="column">
                <h2 class="title is-3">Abstract</h2>
                <div class="content has-text-justified">
                    <p style="font-size: 125%">
                        Prompt-based learning has emerged as a successful paradigm in natural language processing, where
                        a single general-purpose language model can be instructed to perform any task specified by input
                        prompts. Yet task specification in robotics comes in various forms, such as imitating one-shot
                        demonstrations, following language instructions, and reaching visual goals. They are often
                        considered different tasks and tackled by specialized models. This work shows that we can
                        express a wide spectrum of robot manipulation tasks with <i>multimodal prompts</i>, interleaving
                        textual and visual tokens. We design a transformer-based generalist robot agent, VIMA, that
                        processes these prompts and outputs motor actions autoregressively. To train and evaluate VIMA,
                        we develop a new simulation benchmark with thousands of procedurally-generated tabletop tasks
                        with multimodal prompts, 600K+ expert trajectories for imitation learning, and four levels of
                        evaluation protocol for systematic generalization. VIMA achieves strong scalability in both
                        model capacity and data size. It outperforms prior SOTA methods in the hardest zero-shot
                        generalization setting by up to 2.9x task success rate given the same training data. With 10x
                        less training data, VIMA still performs 2.7x better than the top competing approach.
                    </p>
                </div>
            </div>
        </div>
    </div>
</section>

<section class="section">
    <div class="container is-max-widescreen">
        <div class="rows">
            <div class="rows is-centered ">
                <div class="row is-full-width">
                    <img src="media/images/pull.png" class="interpolation-image"
                         alt="" style="display: block; margin-left: auto; margin-right: auto"/>
                    <span style="font-size: 110%"><b>Multimodal prompts for task specification.</b> We observe that many robot manipulation tasks can be expressed as <i>multimodal prompts</i> that interleave language and image/video frames. We propose VIMA, an embodied agent model capable of processing mulitimodal prompts (left) and controlling a robot arm to solve the task (right).</span>
                </div>
            </div>
        </div>
    </div>
</section>

<section class="section">
    <div class="container is-max-widescreen">

        <div class="rows">
            <div class="rows is-centered ">
                <div class="row is-full-width">
                    <h2 class="title is-3"><span
                            class="dvima">VIMA-Bench: Benchmark for Multimodal Robot Learning</span></h2>
                    <span style="font-size: 125%">
                        We provide 17 representative meta-tasks with multimodal prompt templates, which can be procedurally instantiated into thousands of individual tasks by various combinations of textures and tabletop objects.
                    </span>

                    <br>
                    <br>
                    <br>

                    <div class="columns">
                        <!-- Simple Object Manipulation -->
                        <div class="column has-text-left">
                            <h3 class="title is-5">Simple Object Manipulation</h3>
                            <div class="select is-medium">
                                <select id="simple-object-manipulation-menu-tasks"
                                        onchange="updateDemoVideo('simple-object-manipulation')">
                                    <option value="rotate">Rotate</option>
                                    <option value="scene_understanding">Scene Understanding</option>
                                    <option value="simple_manipulation" selected="selected">Visual Manipulation</option>
                                </select>
                            </div>
                            <div class="select is-medium">
                                <select id="simple-object-manipulation-menu-instances"
                                        onchange="updateDemoVideo('simple-object-manipulation')">
                                    <option value="1" selected="selected">01</option>

                                </select>
                            </div>
                            <video id="simple-object-manipulation-single-task-video"
                                   controls
                                   muted
                                   autoplay
                                   loop
                                   width="100%">
                                <source src="/media/videos/demos/simple_object_manipulation/simple_manipulation/1.mp4"
                                        type="video/mp4">
                            </video>
                        </div>

                        <!-- Visual Goal Reaching -->
                        <div class="column has-text-left">
                            <h3 class="title is-5">Visual Goal Reaching</h3>
                            <div class="select is-medium">
                                <select id="visual-goal-reaching-menu-tasks"
                                        onchange="updateDemoVideo('visual-goal-reaching')">
                                    <option value="rearrange" selected="selected">Rearrange</option>
                                    <option value="rearrange_then_restore">Rearrange Then Restore</option>
                                </select>
                            </div>
                            <div class="select is-medium">
                                <select id="visual-goal-reaching-menu-instances"
                                        onchange="updateDemoVideo('visual-goal-reaching')">
                                    <option value="1" selected="selected">01</option>
                                </select>
                            </div>
                            <video id="visual-goal-reaching-single-task-video"
                                   controls
                                   muted
                                   autoplay
                                   loop
                                   width="100%">
                                <source src="/media/videos/demos/visual_goal_reaching/rearrange/1.mp4"
                                        type="video/mp4">
                            </video>
                        </div>


                    </div>

                    <br>
                    <div class="columns">
                        <!-- Novel Concept Grounding -->
                        <div class="column has-text-left">
                            <h3 class="title is-5">Novel Concept Grounding</h3>
                            <div class="select is-medium">
                                <select id="novel-concept-grounding-menu-tasks"
                                        onchange="updateDemoVideo('novel-concept-grounding')">
                                    <option value="novel_adj_and_noun" selected="selected">Novel Adjective and Noun
                                    </option>
                                    <option value="novel_adj">Novel Adjective</option>
                                    <option value="novel_noun">Novel Noun</option>
                                    <option value="twist">Twist</option>
                                </select>
                            </div>
                            <div class="select is-medium">
                                <select id="novel-concept-grounding-menu-instances"
                                        onchange="updateDemoVideo('novel-concept-grounding')">
                                    <option value="1" selected="selected">01</option>
                                </select>
                            </div>
                            <video id="novel-concept-grounding-single-task-video"
                                   controls
                                   muted
                                   autoplay
                                   loop
                                   width="100%">
                                <source src="/media/videos/demos/novel_concept_grounding/novel_adj_and_noun/1.mp4"
                                        type="video/mp4">
                            </video>
                        </div>

                        <!-- One Shot Video Imitation -->
                        <div class="column has-text-left">
                            <h3 class="title is-5">One-shot Video Imitation</h3>
                            <div class="select is-medium">
                                <select id="one-shot-video-imitation-menu-tasks"
                                        onchange="updateDemoVideo('one-shot-video-imitation')">
                                    <option value="follow_motion" selected="selected">Follow Motion</option>
                                    <option value="follow_order">Follow Order</option>
                                </select>
                            </div>
                            <div class="select is-medium">
                                <select id="one-shot-video-imitation-menu-instances"
                                        onchange="updateDemoVideo('one-shot-video-imitation')">
                                    <option value="1" selected="selected">01</option>
                                </select>
                            </div>
                            <video id="one-shot-video-imitation-single-task-video"
                                   controls
                                   muted
                                   autoplay
                                   loop
                                   width="100%">
                                <source src="media/videos/demos/one_shot_video_imitation/follow_motion/1.mp4"
                                        type="video/mp4">
                            </video>
                        </div>
                    </div>

                    <br>
                    <div class="columns">
                        <!-- Visual Constraint Satisfaction -->
                        <div class="column has-text-left">
                            <h3 class="title is-5">Visual Constraint Satisfaction</h3>
                            <div class="select is-medium">
                                <select id="visual-constraint-satisfaction-menu-tasks"
                                        onchange="updateDemoVideo('visual-constraint-satisfaction')">
                                    <option value="sweep_without_exceeding" selected="selected">Sweep without
                                        Exceeding
                                    </option>
                                    <option value="sweep_without_touching">Sweep without Touching</option>
                                </select>
                            </div>
                            <div class="select is-medium">
                                <select id="visual-constraint-satisfaction-menu-instances"
                                        onchange="updateDemoVideo('visual-constraint-satisfaction')">
                                    <option value="1" selected="selected">01</option>
                                </select>
                            </div>
                            <video id="visual-constraint-satisfaction-single-task-video"
                                   controls
                                   muted
                                   autoplay
                                   loop
                                   playbackRate="2.0"
                                   width="100%">
                                <source src="/media/videos/demos/visual_constraint_satisfaction/sweep_without_exceeding/1.mp4"
                                        type="video/mp4">
                            </video>
                        </div>

                        <!-- Visual Reasoning -->
                        <div class="column has-text-left">
                            <h3 class="title is-5">Visual Reasoning</h3>
                            <div class="select is-medium">
                                <select id="visual-reasoning-menu-tasks" onchange="updateDemoVideo('visual-reasoning')">
                                    <option value="manipulate_old_neighbor" selected="selected">Manipulate Old
                                        Neighbor
                                    </option>
                                    <option value="pick_in_order_then_restore">Pick in Order Then Restore</option>
                                    <option value="same_color">Same Texture</option>
                                    <option value="same_profile">Same Shape</option>
                                </select>
                            </div>
                            <div class="select is-medium">
                                <select id="visual-reasoning-menu-instances"
                                        onchange="updateDemoVideo('visual-reasoning')">
                                    <option value="1" selected="selected">01</option>
                                </select>
                            </div>
                            <video id="visual-reasoning-single-task-video"
                                   controls
                                   muted
                                   autoplay
                                   loop
                                   width="100%">
                                <source src="/media/videos/demos/visual_reasoning/manipulate_old_neighbor/1.mp4"
                                        type="video/mp4">
                            </video>
                        </div>
                    </div>
                </div>
            </div>

        </div>
    </div>
</section>

<!--Model-->
<section class="section">
    <div class="container is-max-widescreen">
        <div class="rows">
            <div class="rows is-centered ">
                <div class="row is-full-width">
                    <h2 class="title is-3"><span
                            class="dvima">VIMA: Visuomotor Attention Model</span></h2>
                    <img src="media/images/arch.png" class="interpolation-image"
                         alt="" style="display: block; margin-left: auto; margin-right: auto"/>
                    <span style="font-size: 110%">
<span style="font-weight: bold">VIMA.</span> We encode the multimodal prompts with a pre-trained T5 model, and condition the robot controller on the prompt through cross-attention layers. The controller is a causal transformer decoder consisting of alternating self and cross attention layers that predicts motor commands conditioned on prompts and interaction history.</span>
                </div>
            </div>

        </div>
    </div>
</section>

<!--Experiments-->
<section class="section">
    <div class="container is-max-widescreen">
        <div class="rows">
            <div class="rows is-centered ">
                <div class="row is-full-width">
                    <h2 class="title is-3"><span
                            class="dvima">Experiments</span></h2>

                    <p style="font-size: 125%">
                        We answer three main questions during experiments:
                        (1) How does VIMA compare with prior SOTA transformer-based agents (<a target="_blank"
                                                                                               href="https://www.deepmind.com/publications/a-generalist-agent">Gato</a>,
                        <a target="_blank"
                           href="https://www.deepmind.com/blog/tackling-multiple-tasks-with-a-single-visual-language-model">Flamingo</a>,
                        and <a target="_blank" href="https://arxiv.org/abs/2106.01345">Decision Transformer</a>) on a
                        diverse collection of multimodal-prompted tasks?
                        (2) What are the <span style="font-weight: bold">scaling properties</span> of our approach in
                        model capacity and data size?
                        (3) How do different visual tokenizers, prompt conditioning, and prompt encoding affect decision
                        making?
                    </p>
                    <br>
                    <br>

                    <h3 class="title is-4"><span
                            class="dvima">Evaluation Results</span></h3>

                    <img src="media/images/scalability.png" class="interpolation-image"
                         alt="" style="display: block; margin-left: auto; margin-right: auto"/>
                    <span style="font-size: 110%">
                        <span style="font-weight: bold">Scaling model and data</span>. <i>Top:</i> We compare performance of different methods with model sizes ranging from 2M to 200M parameters. Across all model sizes and generalization levels VIMA outperforms prior works. <i>Bottom:</i> For a fixed model size of 92M parameters we compare the effect of imitation learning dataset size of 0.1%, 1%, 10%, and full imitation data. VIMA is extremely sample efficient and can achieve performance comparable to other methods with 10&times; less data.
                    </span>

                    <br>
                    <br>

                    <h3 class="title is-4"><span
                            class="dvima">Ablation Studies </span></h3>

                    <br>

                    <img src="media/images/ablation_input_process.png" class="interpolation-image"
                         alt="" style="display: block; margin-left: auto; margin-right: auto"/>
                    <span style="font-size: 110%">
                        <span style="font-weight: bold">Ablation on visual tokenizers.</span> We compare the performance of VIMA-200M model across different visual tokenizers. Our proposed object tokens outperform all methods that learn directly from raw pixels, and <i>Object Perceiver</i> that downsamples the object sequence to a fixed number of tokens.
                    </span>

                    <br>
                    <br>
                    <br>
                    <br>

                    <img src="media/images/global_seq_mod.png" class="interpolation-image"
                         alt="" style="display: block; margin-left: auto; margin-right: auto"/>
                    <span style="font-size: 110%">
                        <span style="font-weight: bold">Ablation on prompt conditioning. </span>We compare our method (<i>xattn</i>: cross-attention prompt conditioning) with a vanilla transformer decoder (<i>gpt-decoder</i>) across different model sizes. Cross-attention is especially helpful in low-parameter regime and for harder generalization tasks.
                    </span>
                </div>
            </div>

        </div>
    </div>
</section>

<!--Conclusion-->
<section class="section">
    <div class="container is-max-widescreen">
        <div class="rows">
            <div class="rows is-centered ">
                <div class="row is-full-width">
                    <h2 class="title is-3"><span
                            class="dvima">Conclusion</span></h2>

                    <p style="font-size: 125%">
                        Similar to GPT-3, a generalist robot agent should have an intuitive and expressive interface for
                        human users to convey their intent. In this work, we introduce a novel <i>multimodal</i>
                        prompting formulation that converts diverse robot manipulation tasks into a uniform sequence
                        modeling problem. We propose VIMA, a conceptually simple transformer-based agent capable of
                        solving tasks like visual goal, one-shot video imitation, and novel concept grounding with a
                        single model. VIMA exhibits superior model and data scaling properties, and provides a strong
                        starting point for future work.
                    </p>

                </div>
            </div>

        </div>
    </div>
</section>


<section class="section" id="BibTeX">
    <div class="container is-max-widescreen content">
        <h2 class="title">BibTeX</h2>
        <pre><code>Coming Soon</code></pre>
    </div>
</section>

<footer class="footer">
    <div class="container">
        <div class="columns is-centered">
            <div class="column">
                <div class="content has-text-centered">
                    <p>
                        Website template borrowed from <a
                            href="https://github.com/nerfies/nerfies.github.io">NeRFies</a> and <a
                            href="https://github.com/cliport/cliport.github.io">CLIPort</a>.
                    </p>
                </div>
            </div>
        </div>
    </div>
</footer>

</body>
</html>